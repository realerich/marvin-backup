#!/usr/bin/env python3
"""
æœ¬åœ°æ™ºèƒ½è®°å¿†ç³»ç»Ÿ
æ— éœ€å¤–éƒ¨APIï¼Œä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹å®ç°è¯­ä¹‰æœç´¢
"""

import os
import json
import re
import hashlib
from datetime import datetime
from pathlib import Path

# å°è¯•å¯¼å…¥å‘é‡åº“
try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    VECTOR_LIBS_AVAILABLE = True
except ImportError:
    VECTOR_LIBS_AVAILABLE = False
    print("âš ï¸ å‘é‡åº“æœªå®‰è£…ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")

# é…ç½®
MEMORY_DIR = Path("/root/.openclaw/workspace/memory")
VECTOR_DB_PATH = MEMORY_DIR / "vector_db"
MEMORY_FILE = Path("/root/.openclaw/workspace/MEMORY.md")

class LocalMemory:
    """æœ¬åœ°è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.model = None
        self.embeddings = {}
        self.documents = []
        
        if VECTOR_LIBS_AVAILABLE:
            try:
                # ä½¿ç”¨è½»é‡çº§ä¸­æ–‡æ¨¡å‹
                print("ğŸ”„ åŠ è½½åµŒå…¥æ¨¡å‹...")
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        MEMORY_DIR.mkdir(exist_ok=True)
        VECTOR_DB_PATH.mkdir(exist_ok=True)
        
        # åŠ è½½ç°æœ‰è®°å¿†
        self.load_memory()
    
    def load_memory(self):
        """ä»æ–‡ä»¶åŠ è½½è®°å¿†"""
        # åŠ è½½ MEMORY.md
        if MEMORY_FILE.exists():
            with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
                self._index_document("MEMORY.md", content)
        
        # åŠ è½½ memory/*.md
        for md_file in MEMORY_DIR.glob("*.md"):
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                self._index_document(md_file.name, content)
        
        print(f"ğŸ“š å·²åŠ è½½ {len(self.documents)} ä¸ªè®°å¿†æ–‡æ¡£")
    
    def _index_document(self, filename, content):
        """ç´¢å¼•æ–‡æ¡£"""
        # åˆ†å‰²æˆæ®µè½
        paragraphs = [p.strip() for p in re.split(r'\n\n+', content) if p.strip()]
        
        for i, para in enumerate(paragraphs):
            doc_id = f"{filename}#{i}"
            self.documents.append({
                'id': doc_id,
                'filename': filename,
                'content': para,
                'length': len(para)
            })
            
            # ç”ŸæˆåµŒå…¥
            if self.model:
                self.embeddings[doc_id] = self.model.encode(para)
    
    def search(self, query, top_k=5):
        """æœç´¢è®°å¿†"""
        if not self.documents:
            return []
        
        results = []
        
        if self.model and query in self.embeddings:
            # è¯­ä¹‰æœç´¢
            query_vec = self.model.encode(query)
            
            for doc in self.documents:
                doc_id = doc['id']
                if doc_id in self.embeddings:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = self._cosine_similarity(query_vec, self.embeddings[doc_id])
                    results.append({
                        **doc,
                        'score': float(similarity),
                        'match_type': 'semantic'
                    })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['score'], reverse=True)
            results = results[:top_k]
        
        else:
            # å…³é”®è¯æœç´¢
            query_words = set(query.lower().split())
            
            for doc in self.documents:
                content_lower = doc['content'].lower()
                
                # è®¡ç®—åŒ¹é…åˆ†æ•°
                matches = sum(1 for word in query_words if word in content_lower)
                score = matches / len(query_words) if query_words else 0
                
                if score > 0:
                    results.append({
                        **doc,
                        'score': score,
                        'match_type': 'keyword'
                    })
            
            results.sort(key=lambda x: x['score'], reverse=True)
            results = results[:top_k]
        
        return results
    
    def _cosine_similarity(self, a, b):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def add_memory(self, content, category="general", tags=None):
        """æ·»åŠ æ–°è®°å¿†"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç”Ÿæˆæ–‡ä»¶å
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        filename = f"{timestamp}_{content_hash}.md"
        
        # æ„å»ºè®°å¿†å†…å®¹
        memory_entry = f"""# è®°å¿†æ¡ç›® [{timestamp}]

**ç±»åˆ«**: {category}
**æ ‡ç­¾**: {', '.join(tags or [])}
**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å†…å®¹

{content}

---
*è‡ªåŠ¨è®°å½•*
"""
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        filepath = MEMORY_DIR / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(memory_entry)
        
        # æ›´æ–°ç´¢å¼•
        self._index_document(filename, memory_entry)
        
        print(f"âœ… è®°å¿†å·²ä¿å­˜: {filename}")
        return filename
    
    def extract_facts(self, conversation):
        """ä»å¯¹è¯ä¸­æå–äº‹å®"""
        facts = []
        
        # æå–å…³é”®ä¿¡æ¯æ¨¡å¼
        patterns = [
            (r'æˆ‘å«(\S+)', 'name', 'ç”¨æˆ·åä¸º: {}'),
            (r'æˆ‘å–œæ¬¢(\S+)', 'preference', 'å–œæ¬¢: {}'),
            (r'æˆ‘åœ¨(\S+)', 'location', 'ä½ç½®: {}'),
            (r'(\S+)æ˜¯æˆ‘çš„', 'possession', 'æ‹¥æœ‰: {}'),
            (r'æˆ‘(\S+)å²', 'age', 'å¹´é¾„: {}'),
        ]
        
        for pattern, fact_type, template in patterns:
            matches = re.findall(pattern, conversation)
            for match in matches:
                facts.append({
                    'type': fact_type,
                    'content': template.format(match),
                    'source': 'conversation'
                })
        
        return facts
    
    def summarize_daily(self, date_str=None):
        """ç”Ÿæˆæ¯æ—¥è®°å¿†æ‘˜è¦"""
        if not date_str:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        # æŸ¥æ‰¾å½“å¤©çš„è®°å¿†
        daily_memories = []
        for doc in self.documents:
            if date_str in doc['filename'] or date_str in doc['content']:
                daily_memories.append(doc)
        
        if not daily_memories:
            return f"ğŸ“­ {date_str} æ²¡æœ‰æ–°è®°å¿†"
        
        summary = f"ğŸ“š è®°å¿†æ‘˜è¦ [{date_str}]\n"
        summary += "=" * 50 + "\n\n"
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = {}
        for mem in daily_memories:
            # ä»å†…å®¹ä¸­æå–ç±»åˆ«
            cat_match = re.search(r'\*\*ç±»åˆ«\*\*:\s*(\w+)', mem['content'])
            category = cat_match.group(1) if cat_match else 'general'
            
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(mem)
        
        for category, items in by_category.items():
            summary += f"ğŸ“ {category.upper()} ({len(items)} æ¡)\n"
            for item in items[:3]:  # æ¯ç±»æœ€å¤šæ˜¾ç¤º3æ¡
                content_preview = item['content'][:100].replace('\n', ' ')
                summary += f"  â€¢ {content_preview}...\n"
            if len(items) > 3:
                summary += f"  ... è¿˜æœ‰ {len(items) - 3} æ¡\n"
            summary += "\n"
        
        return summary


def main():
    import sys
    
    memory = LocalMemory()
    
    if len(sys.argv) < 2:
        print("ğŸ§  æœ¬åœ°æ™ºèƒ½è®°å¿†ç³»ç»Ÿ")
        print("\nç”¨æ³•:")
        print("  python3 memory_local.py search <æŸ¥è¯¢>    # æœç´¢è®°å¿†")
        print("  python3 memory_local.py add <å†…å®¹>       # æ·»åŠ è®°å¿†")
        print("  python3 memory_local.py summary [æ—¥æœŸ]   # æ¯æ—¥æ‘˜è¦")
        print("  python3 memory_local.py stats            # ç»Ÿè®¡ä¿¡æ¯")
        sys.exit(1)
    
    cmd = sys.argv[1]
    
    if cmd == 'search':
        query = sys.argv[2] if len(sys.argv) > 2 else input("æœç´¢: ")
        results = memory.search(query)
        
        print(f"\nğŸ” æœç´¢: '{query}'")
        print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:\n")
        
        for i, r in enumerate(results, 1):
            match_icon = "ğŸ”´" if r['score'] > 0.8 else "ğŸŸ¡" if r['score'] > 0.5 else "ğŸŸ¢"
            print(f"{i}. [{match_icon}] {r['filename']} ({r['match_type']}, å¾—åˆ†: {r['score']:.3f})")
            print(f"   {r['content'][:150]}...")
            print()
    
    elif cmd == 'add':
        content = sys.argv[2] if len(sys.argv) > 2 else input("è®°å¿†å†…å®¹: ")
        category = sys.argv[3] if len(sys.argv) > 3 else "general"
        tags = sys.argv[4].split(',') if len(sys.argv) > 4 else []
        
        memory.add_memory(content, category, tags)
    
    elif cmd == 'summary':
        date = sys.argv[2] if len(sys.argv) > 2 else None
        print(memory.summarize_daily(date))
    
    elif cmd == 'stats':
        print("ğŸ“Š è®°å¿†ç»Ÿè®¡")
        print(f"   æ–‡æ¡£æ•°: {len(memory.documents)}")
        print(f"   å‘é‡æ•°: {len(memory.embeddings)}")
        print(f"   å‘é‡åº“å¯ç”¨: {VECTOR_LIBS_AVAILABLE}")
        print(f"   æ¨¡å‹åŠ è½½: {memory.model is not None}")
        
        # æ–‡ä»¶ç»Ÿè®¡
        md_files = list(MEMORY_DIR.glob("*.md"))
        print(f"   è®°å¿†æ–‡ä»¶: {len(md_files)}")
    
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {cmd}")

if __name__ == '__main__':
    main()
